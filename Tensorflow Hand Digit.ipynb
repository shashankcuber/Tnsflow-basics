{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "Xyh2QBlVHogV",
    "outputId": "92198670-e1cb-4c08-d96f-4711e1533ec0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "[[ 3.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 6.]\n",
      " [ 7.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [11.]\n",
      " [12.]]\n",
      "Without placeholder is 6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "con=tf.constant(2.0,name='con')\n",
    "b=tf.placeholder(tf.float32,[None,1],name='b')\n",
    "c=tf.Variable(3.0,name='c')\n",
    "d=tf.add(b,c,name='d')\n",
    "g=tf.multiply(c,con,name='g')\n",
    "\n",
    "#intitalize the tensorflow graph structure\n",
    "init_op=tf.global_variables_initializer()\n",
    "\n",
    "#starting the session\n",
    "with tf.Session() as sess:\n",
    "  #intitialising the variables and opt i.e tensorflow graph\n",
    "  sess.run(init_op)\n",
    "  #getting the otpt\n",
    "  a_out=sess.run(d,feed_dict={b:np.arange(0,10)[:,np.newaxis]})\n",
    "  out=sess.run(g)\n",
    "  print(a_out)\n",
    "  print(\"Without placeholder is {}\".format(out))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "G4QZx6e6Q4Co",
    "outputId": "2e3ed155-30b9-4afc-f1a3-27839008fb35"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8552280febfb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Neural Network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#loading the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#Neural Network\n",
    "#loading the data\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "\n",
    "#optimisation variables\n",
    "learning=0.5\n",
    "epochs=10\n",
    "batch_size=100\n",
    "\n",
    "#declare the training place holders\n",
    "#input x for 28*28 pixels i.e 784\n",
    "\n",
    "x=tf.placeholder(tf.float32,[None,784])\n",
    "\n",
    "#for op data placeholder\n",
    "#10 size for getting output as 0-9 i.e 10 possible digits\n",
    "y=tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "#for weights and bias in the neural network\n",
    "#random_normal is just like randn using normal distribution for generating values\n",
    "#weight connecting the inpute layer to the hidden layer\n",
    "W1=tf.Variable(tf.random_normal([784,300],stddev=0.03),name='W1')\n",
    "b1=tf.Variable(tf.random_normal([300]),name='b1')\n",
    "\n",
    "#weights connecting the hidden layer to output layer\n",
    "W2=tf.Variable(tf.random_normal([300,10],stddev=0.03),name='W2')\n",
    "b2=tf.Variable(tf.random_normal([10]),name='b2')\n",
    "\n",
    "#calculating the output of the hidden layer\n",
    "hidden_out=tf.add(tf.matmul(x,W1),b1)\n",
    "hidden_out=tf.nn.relu(hidden_out)\n",
    "\n",
    "#final output from the hidden layer to the output from the network\n",
    "#softmax is a kind of activation function for output node activation\n",
    "y_output=tf.nn.softmax(tf.add(tf.matmul(hidden_out,W2),b2))\n",
    "\n",
    "#cost/loss function i.e cross entropy cost function\n",
    "\n",
    "#converting y_output to a clipped version limited between 1e-10 to 0.999999 to\n",
    "#avoid the log(0) case during training\n",
    "\n",
    "y_clipped=tf.clip_by_value(y_output,1e-10,0.9999999)\n",
    "cross_entropy=-tf.reduce_mean(tf.reduce_sum(y*tf.log(y_clipped)+(1-y)*tf.log(1-y_clipped),axis=1))\n",
    "\n",
    "#optimiser\n",
    "optimiser=tf.train.GradientDescentOptimizer(learning_rate=learning).minimize(cross_entropy)\n",
    "\n",
    "#initiliaser and acuracy measurement\n",
    "init_op=tf.global_variables_initializer()\n",
    "\n",
    "#accuracy and prediction funcitons\n",
    "\n",
    "prediction=tf.equal(tf.argmax(y,1),tf.argmax(y_output,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(prediction,tf.float32))\n",
    "\n",
    "\n",
    "#setting up the training session in tensorflow\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  #initialise the variables\n",
    "  sess.run(init_op)\n",
    "  #calculating the total no. batches for each training epochs\n",
    "  total_batch=int(len(mnist.train.labels)/batch_size)\n",
    "  #loop for each epoch\n",
    "  for epoch in range(epochs):\n",
    "    #to calculte the cross entropy\n",
    "    avg_cost=0\n",
    "    for i in range(total_batch):\n",
    "      #to select the random batch for training of batch_size=100 using \n",
    "      #next_batch function which automatically allots the batches of data for training\n",
    "        batch_x,batch_y=mnist.train.next_batch(batch_size=batch_size)\n",
    "      #sess.run will perform set of operations so optimiser and cross_entropy so\n",
    "      #for two outputs we assign _ & c \n",
    "        _,c=sess.run([optimiser,cross_entropy],feed_dict={x:batch_x,y:batch_y})\n",
    "      #average cost using cross_entropy\n",
    "        avg_cost+=c/total_batch\n",
    "      #finding the cost after each epoch or training the data for batch_size\n",
    "    print(\"Epoch:\",(epoch+1),\"cost=\",\"{:.3f}\".format(avg_cost))\n",
    "    #finally finding the accuracy of the test and it's approximately 98%\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of Tnsflwbasic.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
